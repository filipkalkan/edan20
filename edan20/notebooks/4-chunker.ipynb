{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment #4: Extracting syntactic groups using machine-learning techniques\n",
    "Author: Pierre Nugues"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this assignment, you will create a system to extract syntactic groups from a text. You will apply it to the CoNLL 2000 dataset. In addition, you will try to link a few extracted named entities to real things using wikipedia."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Objectives"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The objectives of this assignment are to:\n",
    "* Write a program to detect partial syntactic structures\n",
    "* Extract named entities and link them to real things using Wikipedia\n",
    "* Understand the principles of supervised machine learning techniques applied to language processing\n",
    "* Use a popular machine learning toolkit: scikit-learn\n",
    "* Write a short report of 2 to 3 pages on the assignment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Choosing a training and a test sets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As annotated data and annotation scheme, you will use the data available from [CoNLL 2000](https://www.clips.uantwerpen.be/conll2000/chunking/).\n",
    "1. Download both the training and test sets and decompress them.\n",
    "2. Local copies are also available here: [train.txt](https://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/train.txt) and [test.txt](https://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/test.txt)\n",
    "3. Read the description of the CoNLL 2000 task\n",
    "\n",
    "CoNLL 2000 is an early dataset and contrary to many current ones, it has no development set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the corpus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may need to adjust the paths to load the datasets from your machine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "train_file = './conll/train.txt'\r\n",
    "test_file = './conll/test.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Reading the files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read the functions below to load the datasets. They store the corpus in a list of sentences. Each sentence is a list of rows, where each row is a dictionary."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def read_sentences(file):\r\n",
    "    \"\"\"\r\n",
    "    Creates a list of sentences from the corpus\r\n",
    "    Each sentence is a string\r\n",
    "    :param file:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "    f = open(file).read().strip()\r\n",
    "    sentences = f.split('\\n\\n')\r\n",
    "    return sentences"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def split_rows(sentences, column_names):\r\n",
    "    \"\"\"\r\n",
    "    Creates a list of sentence where each sentence is a list of lines\r\n",
    "    Each line is a dictionary of columns\r\n",
    "    :param sentences:\r\n",
    "    :param column_names:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "    new_sentences = []\r\n",
    "    for sentence in sentences:\r\n",
    "        rows = sentence.split('\\n')\r\n",
    "        sentence = [dict(zip(column_names, row.split())) for row in rows]\r\n",
    "        new_sentences.append(sentence)\r\n",
    "    return new_sentences"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading dictionaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The CoNLL 2000 files have three columns: The wordform, `form`, its part of speech, `pos`, and the tag denoting the syntactic group also called the chunk tag, `chunk`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "column_names = ['form', 'pos', 'chunk']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We load the corpus in a list of dictionaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_sentences = read_sentences(train_file)\r\n",
    "train_corpus = split_rows(train_sentences, column_names)\r\n",
    "train_corpus[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline chunker"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most statistical algorithms for language processing start with a so-called baseline. The baseline performance corresponds to the application of a minimal technique that is used to assess the difficulty of a task and for comparison with further programs.\n",
    "\n",
    "You will implement the baseline proposed by the organizers of the\n",
    "        <a href=\"https://www.clips.uantwerpen.be/conll2000/chunking/\">CoNLL 2000 shared task</a>, Sect. <i>Results</i>.\n",
    "1. Read it;\n",
    "2. In the report, you will tell what do you think of it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Auxiliary functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A function to count the parts of speech"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def count_pos(corpus):\r\n",
    "    \"\"\"\r\n",
    "    Computes the part-of-speech distribution\r\n",
    "    in a CoNLL 2000 file\r\n",
    "    :param corpus:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "    pos_cnt = {}\r\n",
    "    for sentence in corpus:\r\n",
    "        for row in sentence:\r\n",
    "            if row['pos'] in pos_cnt:\r\n",
    "                pos_cnt[row['pos']] += 1\r\n",
    "            else:\r\n",
    "                pos_cnt[row['pos']] = 1\r\n",
    "    return pos_cnt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We first collect all the parts of speech and we count them. CoNLL uses the Penn Treebank tagset seen during the course, where `NN` means common noun, singular, `ÌN` means preposition, `DT` means determiner, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "pos_cnt = count_pos(train_corpus)\r\n",
    "pos_cnt"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'NN': 30147,\n",
       " 'IN': 22764,\n",
       " 'DT': 18335,\n",
       " 'VBZ': 4648,\n",
       " 'RB': 6607,\n",
       " 'VBN': 4763,\n",
       " 'TO': 5081,\n",
       " 'VB': 6017,\n",
       " 'JJ': 13085,\n",
       " 'NNS': 13619,\n",
       " 'NNP': 19884,\n",
       " ',': 10770,\n",
       " 'CC': 5372,\n",
       " 'POS': 1769,\n",
       " '.': 8827,\n",
       " 'VBP': 2868,\n",
       " 'VBG': 3272,\n",
       " 'PRP$': 1881,\n",
       " 'CD': 8315,\n",
       " '``': 1531,\n",
       " \"''\": 1493,\n",
       " 'VBD': 6745,\n",
       " 'EX': 206,\n",
       " 'MD': 2167,\n",
       " '#': 36,\n",
       " '(': 274,\n",
       " '$': 1750,\n",
       " ')': 281,\n",
       " 'NNPS': 420,\n",
       " 'PRP': 3820,\n",
       " 'JJS': 374,\n",
       " 'WP': 529,\n",
       " 'RBR': 321,\n",
       " 'JJR': 853,\n",
       " 'WDT': 955,\n",
       " 'WRB': 478,\n",
       " 'RBS': 191,\n",
       " 'PDT': 55,\n",
       " 'RP': 83,\n",
       " ':': 1047,\n",
       " 'FW': 38,\n",
       " 'WP$': 35,\n",
       " 'SYM': 6,\n",
       " 'UH': 15}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chunk distribution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will compute the chunk distribution for each part of speech. You will use the training file to derive the distribution and you will store the results in a dictionary that you will call `chunk_dist`. Below, you have an excerpt of the expected results:\n",
    "```\n",
    "{'JJR':\n",
    "{'I-ADVP': 17, 'I-ADJP': 45, 'I-NP': 204, 'B-ADVP': 63,\n",
    "'B-PP': 2, 'B-ADJP': 111, 'B-NP': 382, 'B-VP': 2,\n",
    "'I-VP': 11, 'O': 16},\n",
    "'CC':\n",
    "{'B-ADVP': 3, 'O': 3676, 'I-VP': 104, 'B-CONJP': 6,\n",
    "'I-ADVP': 30, 'I-UCP': 2, 'I-PP': 24, 'I-ADJP': 26,\n",
    "'I-NP': 1409, 'B-ADJP': 2, 'B-NP': 18, 'B-PP': 70,\n",
    "'I-PRT': 1, 'B-VP': 1},\n",
    "'NN':\n",
    "{'B-LST': 2, 'I-INTJ': 2, 'B-ADVP': 38, 'O': 37,\n",
    "'I-ADVP': 11, 'B-INTJ': 1, 'I-UCP': 2, 'B-UCP': 2,\n",
    "'I-VP': 77, 'B-PRT': 2, 'I-ADJP': 41, 'I-NP': 24456,\n",
    "'B-ADJP': 44, 'B-NP': 5160, 'B-PP': 15, 'B-VP': 257},\n",
    "...\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def get_chunk_dist(train_corpus):\r\n",
    "    chunk_counts = {}\r\n",
    "    for sentence in train_corpus:\r\n",
    "        for row in sentence:\r\n",
    "            pos = row['pos']\r\n",
    "            chunk = row['chunk']\r\n",
    "            if pos not in chunk_counts:\r\n",
    "                chunk_counts[pos] = {}\r\n",
    "\r\n",
    "            if chunk in chunk_counts[pos]:\r\n",
    "                chunk_counts[pos][chunk] += 1\r\n",
    "            else:\r\n",
    "                chunk_counts[pos][chunk] = 1\r\n",
    "    return chunk_counts\r\n",
    "\r\n",
    "chunk_dist = get_chunk_dist(train_corpus)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "chunk_dist['NN']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'B-NP': 5160,\n",
       " 'I-NP': 24456,\n",
       " 'B-VP': 257,\n",
       " 'B-ADJP': 44,\n",
       " 'B-ADVP': 38,\n",
       " 'O': 37,\n",
       " 'B-PP': 15,\n",
       " 'I-ADVP': 11,\n",
       " 'I-ADJP': 41,\n",
       " 'I-VP': 77,\n",
       " 'B-INTJ': 1,\n",
       " 'B-LST': 2,\n",
       " 'B-UCP': 2,\n",
       " 'I-UCP': 2,\n",
       " 'B-PRT': 2,\n",
       " 'I-INTJ': 2}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Selecting the POS-chunk associations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each part of speech, select the best association. In the example above, you will have (NN, I-NP) as it is the most frequent association. You will store the results in a dictionary that you will call `pos_chunk`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "pos_chunk = {}\r\n",
    "for pos in chunk_dist:\r\n",
    "    max_count = 0\r\n",
    "    for chunk in chunk_dist[pos]:\r\n",
    "        chunk_count = chunk_dist[pos][chunk]\r\n",
    "        if chunk_count > max_count:\r\n",
    "            max_count = chunk_count\r\n",
    "            pos_chunk[pos] = chunk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "pos_chunk['NN']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'I-NP'"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "pos_chunk['JJR']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'B-NP'"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the resulting associations, apply your chunker to the test file. You will write a `predict(model, corpus)` function, where `model` will be your associations and `corpus`, the test corpus. You will format the test corpus as a dictionary, where you will add a `pchunk` key for each row with a value that will correspond to the predicted chunk (`p` is for _predicted_)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def predict(model, corpus):\r\n",
    "    for sentence in corpus:\r\n",
    "        for row in sentence:\r\n",
    "            pos = row['pos']\r\n",
    "            row['pchunk'] = model[pos]\r\n",
    "    return corpus"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We load the test corpus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "test_sentences = read_sentences(test_file)\r\n",
    "test_corpus = split_rows(test_sentences, column_names)\r\n",
    "test_corpus[:1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We predict the groups. You should have added a `pchunk` key"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "predicted_test_corpus = predict(pos_chunk, test_corpus)\r\n",
    "predicted_test_corpus[:1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}]]"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can evaluate the performance of the baseline with the tag accuracy: the percentage of words that receive the correct tag."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "def eval(predicted):\r\n",
    "    \"\"\"\r\n",
    "    Evaluates the predicted chunk accuracy\r\n",
    "    :param predicted:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "    word_cnt = 0\r\n",
    "    correct = 0\r\n",
    "    for sentence in predicted:\r\n",
    "        for row in sentence:\r\n",
    "            word_cnt += 1\r\n",
    "            if row['chunk'] == row['pchunk']:\r\n",
    "                correct += 1\r\n",
    "    return correct / word_cnt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "accuracy = eval(predicted_test_corpus)\r\n",
    "accuracy"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7729066846782194"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The CoNLL evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The accuracy is very misleading as it is biased by the most frequent tags. It is not a good way to evaluate chunking. Instead, CoNLL computes the F1 score of all the chunks with a specific evaluation script. This F1 score is the harmonic mean of precision and recall."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Saving the corpus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use the CoNLL evaluation script, you will store your results in an output file that has four columns. The three first columns will be the input columns from the test file: \n",
    "* word, \n",
    "* part of speech, and \n",
    "* gold-standard chunk. \n",
    "\n",
    "You will append the predicted chunk as the 4th column. Your output file should look like the excerpt below:\n",
    "```\n",
    "Rockwell NNP B-NP I-NP\n",
    "International NNP I-NP I-NP\n",
    "Corp. NNP I-NP I-NP\n",
    "'s POS B-NP B-NP\n",
    "Tulsa NNP I-NP I-NP\n",
    "unit NN I-NP I-NP\n",
    "said VBD B-VP B-VP\n",
    "it PRP B-NP B-NP\n",
    "```\n",
    "In this corpus, the column separator is the space. In most recent corpora, the separator would be a tabulation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will use a `save_results(output_dict, keys, output_file)` function, where the keys will be `['form', 'pos', 'chunk', 'pchunk']`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "keys = ['form', 'pos', 'chunk', 'pchunk']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "def save_results(output_dict, keys, output_file):\r\n",
    "    f_out = open(output_file, 'w')\r\n",
    "    # We write the word (form), part of speech (pos),\r\n",
    "    # gold-standard chunk (chunk), and predicted chunk (pchunk)\r\n",
    "    for sentence in output_dict:\r\n",
    "        for row in sentence:\r\n",
    "            for key in keys:\r\n",
    "                f_out.write(row[key] + ' ')\r\n",
    "            f_out.write('\\n')\r\n",
    "        f_out.write('\\n')\r\n",
    "    f_out.close()\r\n",
    "    return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "save_results(predicted_test_corpus, keys, 'out')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The CoNLL 2000 evaluation script will use these two last columns, chunk and predicted chunk, to compute the performance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation Procedure"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To evaluate your results, you have two options:\n",
    "1. Use the original conlleval script here:  <a href=\"https://www.clips.uantwerpen.be/conll2000/chunking/\"><tt>conlleval.txt</tt></a>.\n",
    "2. Use a Python translation of it. \n",
    "\n",
    "You will use the second option and you will describe the results you obtained in your report"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The Python translation of the `conlleval`script"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the cell below to install the script from https://github.com/kaniblu/conlleval:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "!pip install conlleval"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting conlleval\n",
      "  Downloading conlleval-0.2-py3-none-any.whl (5.4 kB)\n",
      "Installing collected packages: conlleval\n",
      "Successfully installed conlleval-0.2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compute the baseline score. Check that this score corresponds to the one reported in the CoNLL description."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "import conlleval\r\n",
    "lines = open('out').read().splitlines()\r\n",
    "res = conlleval.evaluate(lines)\r\n",
    "baseline_score = res['overall']['chunks']['evals']['f1']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "baseline_score"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.770671072299583"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The official script"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may want to double-check your results with the original CoNLL script. It is more complex to use and this part is optional. I suggest not to try it before you are done with the program.\n",
    "\n",
    "To run the original script, read these items:\n",
    "* <tt>conlleval.txt</tt> is the official CoNLL Perl script. It expects the two last columns of the test set to be the manually assigned chunk (gold standard) and the predicted chunk.\n",
    "* <tt>conlleval.txt</tt> was written for Unix and if you run Windows, you will have to use a terminal command. In the File menu of the notebook, select New and then Terminal.\n",
    "* Start it like this: ` $ conlleval.txt <out` where the `out` file contains both the gold and predicted chunk tags. `conlleval.txt` is a Perl script.\n",
    "* Perl is installed on most Unix distributions. If it is not installed on your machine, you need to install it. Make also sure that you have the execution rights. Otherwise change them with: `$ chmod +x conlleval.txt`\n",
    "* The `conlleval.txt` script expects the new lines to be `\\n` as in Unix. If you run your Python program on Windows, your new lines will be `\\r\\n`. To have the correct new lines, add this parameter to `open()`: `newline='\\n’` like this: `f_out = open('out', ‘w’, newline='\\n’)`\n",
    "* The complete description of the CoNLL 2000 evaluation script is available here: [https://www.clips.uantwerpen.be/conll2000/chunking/output.html](https://www.clips.uantwerpen.be/conll2000/chunking/output.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Machine Learning: A first ML program"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise, you will apply and explore a machine-learning program.\n",
    "\n",
    "The program that won the CoNLL 2000 shared task (Kudoh and Matsumoto, 2000) used a window of five words around the chunk tag to identify, $c_i$. They built a feature vector consisting of:\n",
    "1. The values of the five words in this window: $w_{i-2}, w_{i-1}, w_{i}, w_{i+1}, w_{i+2}$\n",
    "2. The values of the five parts of speech in this window: $t_{i-2}, t_{i-1}, t_{i}, t_{i+1}, t_{i+2}$\n",
    "3. The values of the two previous chunk tags in the first part of the window: $c_{i-2}, c_{i-1}$\n",
    "\n",
    "The two last parameters (3.) are said to be dynamic because the program computes them at run-time. Read [Kudoh and Matsumoto's paper](https://www.clips.uantwerpen.be/conll2000/pdf/14244kud.pdf) and the Yamcha (http://www.chasen.org/~taku/software/yamcha/) software site. We would call this architecture recurrent now.\n",
    "\n",
    "You will start with a given code that uses the two first sets of features (1. and 2.) and add yourself the last one (3.) to improve the performance of your chunker. Kudoh and Matsumoto trained a classifier based on support vector machines. You will use logistic regression."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "import bs4\r\n",
    "import os\r\n",
    "import requests\r\n",
    "from sklearn.feature_extraction import DictVectorizer\r\n",
    "from sklearn import svm\r\n",
    "from sklearn import linear_model\r\n",
    "from sklearn import metrics\r\n",
    "from sklearn import tree\r\n",
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A first function to extract features from one sentence"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "def extract_features_sent_static(sentence, w_size, feature_names):\r\n",
    "    \"\"\"\r\n",
    "    Extract the features from one sentence\r\n",
    "    returns X and y, where X is a list of dictionaries and\r\n",
    "    y is a list of symbols\r\n",
    "    :param sentence: string containing the CoNLL structure of a sentence\r\n",
    "    :param w_size:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # We pad the sentence to extract the context window more easily\r\n",
    "    start = [{'form': '__bos__', 'pos': '__bos__', 'chunk': '__bos__'}]\r\n",
    "    end = [{'form': '__eos__', 'pos': '__eos__', 'chunk': '__eos__'}]\r\n",
    "    start *= w_size\r\n",
    "    end *= w_size\r\n",
    "    padded_sentence = start + sentence\r\n",
    "    padded_sentence += end\r\n",
    "\r\n",
    "    # We extract the features and the classes\r\n",
    "    # X contains is a list of features, where each feature vector is a dictionary\r\n",
    "    # y is the list of classes\r\n",
    "    X = list()\r\n",
    "    y = list()\r\n",
    "    for i in range(len(padded_sentence) - 2 * w_size):\r\n",
    "        # x is a row of X\r\n",
    "        x = list()\r\n",
    "        # The words in lower case\r\n",
    "        for j in range(2 * w_size + 1):\r\n",
    "            x.append(padded_sentence[i + j]['form'].lower())\r\n",
    "        # The POS\r\n",
    "        for j in range(2 * w_size + 1):\r\n",
    "            x.append(padded_sentence[i + j]['pos'])\r\n",
    "        # The chunks (Up to the word)\r\n",
    "        \"\"\"\r\n",
    "        for j in range(w_size):\r\n",
    "            feature_line.append(padded_sentence[i + j]['chunk'])\r\n",
    "        \"\"\"\r\n",
    "        # We represent the feature vector as a dictionary\r\n",
    "        X.append(dict(zip(feature_names, x)))\r\n",
    "        # The classes are stored in a list\r\n",
    "        y.append(padded_sentence[i + w_size]['chunk'])\r\n",
    "    return X, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And from all the sentences"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "def extract_features_static(sentences, w_size, feature_names):\r\n",
    "    \"\"\"\r\n",
    "    Builds X matrix and y vector\r\n",
    "    X is a list of dictionaries and y is a list\r\n",
    "    :param sentences:\r\n",
    "    :param w_size:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "    X_l = []\r\n",
    "    y_l = []\r\n",
    "    for sentence in sentences:\r\n",
    "        X, y = extract_features_sent_static(sentence, w_size, feature_names)\r\n",
    "        X_l.extend(X)\r\n",
    "        y_l.extend(y)\r\n",
    "    return X_l, y_l"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Applying the feature extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The size of the window and the names of the features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "w_size = 2  # The size of the context window to the left and right of the word\r\n",
    "feature_names = ['word_n2', 'word_n1', 'word', 'word_p1', 'word_p2',\r\n",
    "                 'pos_n2', 'pos_n1', 'pos', 'pos_p1', 'pos_p2']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We read the corpus and format it as a dictionary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "train_sentences = read_sentences(train_file)\r\n",
    "train_corpus = split_rows(train_sentences, column_names)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "train_corpus[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "X_dict, y = extract_features_static(train_corpus, w_size, feature_names)\r\n",
    "X_dict[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'word_n2': '__bos__',\n",
       "  'word_n1': '__bos__',\n",
       "  'word': 'confidence',\n",
       "  'word_p1': 'in',\n",
       "  'word_p2': 'the',\n",
       "  'pos_n2': '__bos__',\n",
       "  'pos_n1': '__bos__',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': 'IN',\n",
       "  'pos_p2': 'DT'},\n",
       " {'word_n2': '__bos__',\n",
       "  'word_n1': 'confidence',\n",
       "  'word': 'in',\n",
       "  'word_p1': 'the',\n",
       "  'word_p2': 'pound',\n",
       "  'pos_n2': '__bos__',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': 'IN',\n",
       "  'pos_p1': 'DT',\n",
       "  'pos_p2': 'NN'}]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "y[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['B-NP', 'B-PP']"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature encoding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# Vectorize the feature matrix and carry out a one-hot encoding\r\n",
    "vec = DictVectorizer(sparse=True)\r\n",
    "X = vec.fit_transform(X_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "classifier = linear_model.LogisticRegression()\r\n",
    "model = classifier.fit(X, y)\r\n",
    "model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\filip\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predicting the test set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We read the sentences and create a dictionary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "test_sentences = read_sentences(test_file)\r\n",
    "test_corpus = split_rows(test_sentences, column_names)\r\n",
    "test_corpus[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We extract the features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "X_test_dict, y_test = extract_features_static(test_corpus, w_size, feature_names)\r\n",
    "X_test_dict[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'word_n2': '__bos__',\n",
       "  'word_n1': '__bos__',\n",
       "  'word': 'rockwell',\n",
       "  'word_p1': 'international',\n",
       "  'word_p2': 'corp.',\n",
       "  'pos_n2': '__bos__',\n",
       "  'pos_n1': '__bos__',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'NNP'},\n",
       " {'word_n2': '__bos__',\n",
       "  'word_n1': 'rockwell',\n",
       "  'word': 'international',\n",
       "  'word_p1': 'corp.',\n",
       "  'word_p2': \"'s\",\n",
       "  'pos_n2': '__bos__',\n",
       "  'pos_n1': 'NNP',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'POS'}]"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "y_test[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['B-NP', 'I-NP']"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We vectorize the features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "X_test = vec.transform(X_test_dict)  # Possible to add: .toarray()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we predict the test set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "y_test_predicted = classifier.predict(X_test)\r\n",
    "y_test_predicted[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['B-NP', 'I-NP'], dtype='<U7')"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now add the predicted chunks to the sentences"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "inx = 0\r\n",
    "for sentence in test_corpus:\r\n",
    "    for word in sentence:\r\n",
    "        word['pchunk'] = y_test_predicted[inx]\r\n",
    "        inx += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The index should be equal to the length of the prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "print(inx)\r\n",
    "len(y_test_predicted)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "47377\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "47377"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "test_corpus[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}],\n",
       " [{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR', 'pchunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}]]"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "save_results(test_corpus, keys, 'out')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluating the performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "lines = open('out').read().splitlines()\r\n",
    "res = conlleval.evaluate(lines)\r\n",
    "simple_ml_score = res['overall']['chunks']['evals']['f1']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "simple_ml_score"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9157688948047087"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions on the ML program"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Please read these questions and answer them in your report:\n",
    "1. What is the feature vector that corresponds to the chunking program above? Is it the same Kudoh and Matsumoto used in their experiment?\n",
    "2. What is the performance of the chunker?\n",
    "3. Remove the lexical features (the words) from the feature vector and measure the performance. You should observe a decrease.\n",
    "4. What is the classifier used in the program? \n",
    "5. As an optional task, you may increase the number of iterations, try two other classifiers from sklearn and measure their performance: decision trees, perceptron, support vector machines, etc. Be aware that support vector machines take a long time to train: up to one hour."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Machine Learning: Adding all the features from Kudoh and Matsumoto"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Complement the feature vector used in the previous section with the two dynamic features, $c_{i-2}, c_{i-1}$, and train a new model. You will need to write a new `extract_features_sent_dyn` and `predict` functions. \n",
    "In his experiments, your teacher obtained a F1 score of 92.65 with logistic regression and the default parameters from sklearn, i.e. `linear_model.LogisticRegression()`;"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**A frequent mistake in the labs** is to use the gold-standard chunks from the test set. Be aware that  when you predict the test set, you do not know the dynamic features in advance and you must  not use the ones from the test file. You will use the two previous chunk tags that you have predicted."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You need to reach a global F1 score of 92 to pass this laboratory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write an `extract_features_sent_dyn(sentence, w_size, feature_names)` function to extract the features from one sentence."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "def extract_features_sent_dyn(sentence, w_size, feature_names):\r\n",
    "    \"\"\"\r\n",
    "    Extract the features from one sentence\r\n",
    "    returns X and y, where X is a list of dictionaries and\r\n",
    "    y is a list of symbols\r\n",
    "    :param sentence: string containing the CoNLL structure of a sentence\r\n",
    "    :param w_size:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # We pad the sentence to extract the context window more easily\r\n",
    "    start = [{'form': '__bos__', 'pos': '__bos__', 'chunk': '__bos__'}]\r\n",
    "    end = [{'form': '__eos__', 'pos': '__eos__', 'chunk': '__eos__'}]\r\n",
    "    start *= w_size\r\n",
    "    end *= w_size\r\n",
    "    padded_sentence = start + sentence\r\n",
    "    padded_sentence += end\r\n",
    "\r\n",
    "    # We extract the features and the classes\r\n",
    "    # X contains is a list of features, where each feature vector is a dictionary\r\n",
    "    # y is the list of classes\r\n",
    "    X = list()\r\n",
    "    y = list()\r\n",
    "    for i in range(len(padded_sentence) - 2 * w_size):\r\n",
    "        # x is a row of X\r\n",
    "        feature_line = list()\r\n",
    "        # The words in lower case\r\n",
    "        for j in range(2 * w_size + 1):\r\n",
    "            feature_line.append(padded_sentence[i + j]['form'].lower())\r\n",
    "        # The POS\r\n",
    "        for j in range(2 * w_size + 1):\r\n",
    "            feature_line.append(padded_sentence[i + j]['pos'])\r\n",
    "        # The chunks (Up to the word)\r\n",
    "        for j in range(w_size):\r\n",
    "            feature_line.append(padded_sentence[i + j]['chunk'])\r\n",
    "        # We represent the feature vector as a dictionary\r\n",
    "        X.append(dict(zip(feature_names, feature_line)))\r\n",
    "        # The classes are stored in a list\r\n",
    "        y.append(padded_sentence[i + w_size]['chunk'])\r\n",
    "    return X, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We apply `extract_features_sent_dyn` to all the sentences"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "def extract_features_dyn(sentences, w_size, feature_names):\r\n",
    "    \"\"\"\r\n",
    "    Builds X matrix and y vector\r\n",
    "    X is a list of dictionaries and y is a list\r\n",
    "    :param sentences:\r\n",
    "    :param w_size:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "    X_l = []\r\n",
    "    y_l = []\r\n",
    "    for sentence in sentences:\r\n",
    "        X, y = extract_features_sent_dyn(sentence, w_size, feature_names)\r\n",
    "        X_l.extend(X)\r\n",
    "        y_l.extend(y)\r\n",
    "    return X_l, y_l"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "feature_names_dyn = ['word_n2', 'word_n1', 'word', 'word_p1', 'word_p2',\r\n",
    "                     'pos_n2', 'pos_n1', 'pos', 'pos_p1', 'pos_p2', 'chunk_n2',\r\n",
    "                     'chunk_n1']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "train_sentences = read_sentences(train_file)\r\n",
    "train_corpus = split_rows(train_sentences, column_names)\r\n",
    "train_corpus[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "X_dict, y = extract_features_dyn(train_corpus, w_size, feature_names_dyn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "X_dict[:3]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'word_n2': '__bos__',\n",
       "  'word_n1': '__bos__',\n",
       "  'word': 'confidence',\n",
       "  'word_p1': 'in',\n",
       "  'word_p2': 'the',\n",
       "  'pos_n2': '__bos__',\n",
       "  'pos_n1': '__bos__',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': 'IN',\n",
       "  'pos_p2': 'DT',\n",
       "  'chunk_n2': '__bos__',\n",
       "  'chunk_n1': '__bos__'},\n",
       " {'word_n2': '__bos__',\n",
       "  'word_n1': 'confidence',\n",
       "  'word': 'in',\n",
       "  'word_p1': 'the',\n",
       "  'word_p2': 'pound',\n",
       "  'pos_n2': '__bos__',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': 'IN',\n",
       "  'pos_p1': 'DT',\n",
       "  'pos_p2': 'NN',\n",
       "  'chunk_n2': '__bos__',\n",
       "  'chunk_n1': 'B-NP'},\n",
       " {'word_n2': 'confidence',\n",
       "  'word_n1': 'in',\n",
       "  'word': 'the',\n",
       "  'word_p1': 'pound',\n",
       "  'word_p2': 'is',\n",
       "  'pos_n2': 'NN',\n",
       "  'pos_n1': 'IN',\n",
       "  'pos': 'DT',\n",
       "  'pos_p1': 'NN',\n",
       "  'pos_p2': 'VBZ',\n",
       "  'chunk_n2': 'B-NP',\n",
       "  'chunk_n1': 'B-PP'}]"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will now vectorize the training set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "vec = DictVectorizer(sparse=True)\r\n",
    "X = vec.fit_transform(X_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And create and fit the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "classifier = linear_model.LogisticRegression()\r\n",
    "model = classifier.fit(X, y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\filip\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will finally predict the test set. We load the corpus again."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "test_sentences = read_sentences(test_file)\r\n",
    "test_corpus = split_rows(test_sentences, column_names)\r\n",
    "test_corpus[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us extract the static features from one sentence"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "X_test_dict, y_test = extract_features_static([test_corpus[0]], w_size, feature_names)\r\n",
    "X_test_dict[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'word_n2': '__bos__',\n",
       "  'word_n1': '__bos__',\n",
       "  'word': 'rockwell',\n",
       "  'word_p1': 'international',\n",
       "  'word_p2': 'corp.',\n",
       "  'pos_n2': '__bos__',\n",
       "  'pos_n1': '__bos__',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'NNP'},\n",
       " {'word_n2': '__bos__',\n",
       "  'word_n1': 'rockwell',\n",
       "  'word': 'international',\n",
       "  'word_p1': 'corp.',\n",
       "  'word_p2': \"'s\",\n",
       "  'pos_n2': '__bos__',\n",
       "  'pos_n1': 'NNP',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'POS'}]"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This $\\mathbf{X}\\_{\\textrm{dict}}$ is incomplete. For the prediction, we need to reinject dynamically the two previously predicted tags to have the full feature vector. Write this code here. \r\n",
    "\r\n",
    "This part is probably the most difficult of the lab. You may want to write it first for one sentence, and then for the test corpus. The prediction will take a longer time and you may want to include a progress bar with this snippet: \r\n",
    "```\r\n",
    "from tqdm import tqdm\r\n",
    "for test_sentence in tqdm(test_corpus):\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "y_test_predicted_dyn = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "from tqdm import tqdm\r\n",
    "y_test_predicted_dyn = []\r\n",
    "for sentence in tqdm(test_corpus):\r\n",
    "    X_test_dict, y_test = extract_features_static([sentence], w_size, feature_names)\r\n",
    "        \r\n",
    "    chunk_n2 = '__bos__'\r\n",
    "    chunk_n1 = '__bos__'\r\n",
    "    \r\n",
    "    for x_test in X_test_dict:\r\n",
    "        x_test['chunk_n2'], x_test['chunk_n1'] = chunk_n2, chunk_n1\r\n",
    "        \r\n",
    "        x_test_vec = vec.transform(x_test)\r\n",
    "        this_prediction = classifier.predict(x_test_vec)[0]\r\n",
    "        \r\n",
    "        chunk_n2, chunk_n1 = chunk_n1, this_prediction\r\n",
    "        \r\n",
    "        y_test_predicted_dyn.append(this_prediction)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2012/2012 [05:25<00:00,  6.18it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "y_test_predicted_dyn[:3]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['B-NP', 'I-NP', 'I-NP']"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "inx = 0\r\n",
    "for sentence in test_corpus:\r\n",
    "    for word in sentence:\r\n",
    "        word['pchunk'] = y_test_predicted_dyn[inx]\r\n",
    "        inx += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "save_results(test_corpus, keys, 'out')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "lines = open('out').read().splitlines()\r\n",
    "res = conlleval.evaluate(lines)\r\n",
    "improved_ml_score = res['overall']['chunks']['evals']['f1']\r\n",
    "improved_ml_score"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9265266775640221"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optional improvement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As an optional task, you can try to improve the score with beam search. If you know this technique, apply it using the probability output of logistic regression.\n",
    "\n",
    "With the same classifier and a beam diameter of 5, your teacher obtained 92.87."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collecting the entities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will now collect all the named entities from the training set meeting the two conditions:\n",
    "1. Defined as NP chunks and \n",
    "2. Starting with a `NNP` (proper noun) or a `NNPS` (proper noun, plural) tag. \n",
    "\n",
    "As an example, in the first sentence of `train_corpus`, you will extract `('September', )` and `('July', 'and', 'August')`. You will set all the tuples in a set that you will call `ne_set`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "train_corpus[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       " {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       " {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       " {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       " {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       " {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       " {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       " {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       " {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       " {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       " {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       " {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       " {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       " {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       " {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       " {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       " {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       " {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       " {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       " {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       " {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       " {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       " {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       " {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       " {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       " {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       " {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       " {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       " {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       " {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       " {'form': '.', 'pos': '.', 'chunk': 'O'}]"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can write a two-pass procedure. For each sentence of the corpus:\n",
    "1. In the first pass, you will collect the start indices of the noun groups (starting with a `B-NP`) which are also proper nouns (`NNP`or `NNPS`). For the first sentence, it will result in the list `[16, 30]`;\n",
    "2. In the second pass, you will collect the segments (`B-NP` followed by a sequence of `I-NP`), starting at each index. For the first sentence, it will result in the tuples `('September',)`and `('July', 'and', 'August')`\n",
    "\n",
    "Should you have a better solution, please use it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "ne_set = set()\r\n",
    "for sentence in train_corpus:\r\n",
    "    i = 0\r\n",
    "    while i < len(sentence):\r\n",
    "        row = sentence[i]\r\n",
    "        pos = row['pos']\r\n",
    "        chunk = row['chunk']\r\n",
    "        if chunk == 'B-NP' and (pos == 'NNP' or pos == 'NNPS'):\r\n",
    "            sequence = [row['form']]\r\n",
    "            if i+1 < len(sentence):\r\n",
    "                i += 1\r\n",
    "            else:\r\n",
    "                break\r\n",
    "            while sentence[i]['chunk'] == 'I-NP':\r\n",
    "                sequence.append(sentence[i]['form'])\r\n",
    "                if i+1 < len(sentence):\r\n",
    "                    i += 1\r\n",
    "                else:\r\n",
    "                    break\r\n",
    "            ne_set.add(tuple(sequence))\r\n",
    "        else:\r\n",
    "            i += 1\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "len(ne_set)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4348"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "list(sorted(ne_set))[:25]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('1-2-3', 'Release', '3'),\n",
       " ('67-year-old', 'Mrs.', 'Thi'),\n",
       " ('A.', 'Foster', 'Higgins', '&', 'Co.'),\n",
       " ('A.', 'Schulman'),\n",
       " ('A.F.', 'Delchamps', 'Jr.'),\n",
       " ('A.G.', 'Edwards', '&', 'Sons', 'Inc.'),\n",
       " ('A.P.', 'Green'),\n",
       " ('A.P.', 'Green', 'Industries'),\n",
       " ('A.P.', 'Green', 'Industries', 'Inc.'),\n",
       " ('ABC',),\n",
       " ('AC&R', 'ADVERTISING'),\n",
       " ('AC&R', 'Advertising'),\n",
       " ('AC&R\\\\/CCL', 'Advertising'),\n",
       " ('ADN',),\n",
       " ('AEW',),\n",
       " ('AGS', 'Computers'),\n",
       " ('AMR',),\n",
       " ('AMR', 'Corp.'),\n",
       " ('AMR', 'shareholders'),\n",
       " ('AMRO', 'Bank'),\n",
       " ('ANB', 'Investment', 'Management', 'Co.'),\n",
       " ('AON',),\n",
       " ('ARA', 'Services', 'Inc.'),\n",
       " ('ARCO',),\n",
       " ('ARTY', 'FAX')]"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a small set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To run the subsequent experiments faster, you will limit the dataset to the entities starting with letter _K_. I chose this letter, because it corresponded to one of the smallest sets. You will call the resulting set: `ne_small_set`. Feel free to use the full set after you have completed this assignment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "ne_small_set = set(filter(lambda sequence: str(sequence[0]).startswith('K'), ne_set))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "sorted(ne_small_set)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('K', 'mart', 'Corp.', 'Chairman', 'Joseph', 'E.', 'Antonini'),\n",
       " ('K-H', 'Corp.'),\n",
       " ('KIM',),\n",
       " ('KLM', 'Royal', 'Dutch', 'Airlines'),\n",
       " ('KPMG', 'Peat', 'Marwick'),\n",
       " ('KTXL',),\n",
       " ('Kabul',),\n",
       " ('Kacy', 'McClelland'),\n",
       " ('Kaitaia',),\n",
       " ('Kajima',),\n",
       " ('Kansas',),\n",
       " ('Kansas', 'Power'),\n",
       " ('Kansas', 'and', 'Texas'),\n",
       " ('Karen', 'Olshan'),\n",
       " ('Kary', 'Moss'),\n",
       " ('Kasler', 'Corp.'),\n",
       " ('Kate', 'Michelman'),\n",
       " ('Kathe', 'Dillmann'),\n",
       " ('Kathie', 'Huff'),\n",
       " ('Kathie', 'Roberts'),\n",
       " ('Kathryn', 'McGrath'),\n",
       " ('Kathy', 'Stanwick'),\n",
       " ('Katonah',),\n",
       " ('Kawasaki', 'Steel'),\n",
       " ('Ke', 'Zaishuo'),\n",
       " ('Kean', 'forces'),\n",
       " ('Keefe', ',', 'Bruyette', '&', 'Woods', 'Inc.'),\n",
       " ('Keihin', 'Electric', 'Express', 'Railway', 'Co'),\n",
       " ('Keith', 'L.', 'Fogg'),\n",
       " ('Keith', 'Mulrooney'),\n",
       " ('Keizaikai',),\n",
       " ('Keizaikai', 'Corp.'),\n",
       " ('Kemper',),\n",
       " ('Kenneth', 'A.', 'Eldred'),\n",
       " ('Kenneth', 'Abraham'),\n",
       " ('Kenneth', 'H.', 'Olsen'),\n",
       " ('Kenneth', 'T.', 'Rosen'),\n",
       " ('Kent', 'Neal'),\n",
       " ('Kenton',),\n",
       " ('Kentucky',),\n",
       " ('Kentucky', 'Fried', 'Chicken', 'stores'),\n",
       " ('Kevin', 'Logan'),\n",
       " ('Khmer', 'Rouge', 'camps'),\n",
       " ('Khost',),\n",
       " ('Kidder', ',', 'Peabody', '&', 'Co'),\n",
       " ('Kidder', ',', 'Peabody', '&', 'Co.'),\n",
       " ('Kidder', 'Peabody'),\n",
       " ('Kim', 'Schwartz'),\n",
       " ('Kimberly', 'Ann', 'Smith'),\n",
       " ('King', 'Broadcasting', 'Co.'),\n",
       " ('Kinney', 'and', 'Foot', 'Locker', 'shoe', 'stores'),\n",
       " ('Kirin',),\n",
       " ('Kirin', 'Brewery'),\n",
       " ('Kitchen',),\n",
       " ('Kleinwort', 'Benson', 'Government', 'Securities', 'Inc.'),\n",
       " ('Kleinwort', 'Benson', 'North', 'America'),\n",
       " ('Knoxville',),\n",
       " ('Ko', 'Shioya'),\n",
       " ('Kobe', 'Steel'),\n",
       " ('Kobe', 'Steel', 'Ltd.'),\n",
       " ('Kodak',),\n",
       " ('Kollmorgen',),\n",
       " ('Kraft', 'General', 'Foods'),\n",
       " ('Kremlin', 'wrangling'),\n",
       " ('Krenz',),\n",
       " ('Kringle', 'fares'),\n",
       " ('Kroger', 'Co'),\n",
       " ('Kuala', 'Lumpur'),\n",
       " ('Kumagai-Gumi',),\n",
       " ('Kurds',),\n",
       " ('Kursk', 'and', 'Smolensk'),\n",
       " ('Kurt', 'Hager'),\n",
       " ('Kwek', 'Hong', 'Png'),\n",
       " ('Ky',),\n",
       " ('Ky.',),\n",
       " ('Kyocera',)]"
      ]
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Resolving the entities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will now implement a simple method to link the named entities from the previous exercise to unique pages  or identifiers in Wikipedia and Wikidata."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, look at a few entities in your small set and find:\n",
    "1. A few entities that you think are in wikipedia, \n",
    "2. Entities that will not be in wikipedia, and \n",
    "3. Entities that you think are ambiguous: An entity that may correspond to two or more things. \n",
    "\n",
    "You will describe your findings in the report."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A function to lookup entities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read the function below and try to understand what it means. You will describe it in your report."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "def wikipedia_lookup(ner, base_url='https://en.wikipedia.org/wiki/'):\r\n",
    "    try:\r\n",
    "        url_en = base_url + ' '.join(ner)\r\n",
    "        html_doc = requests.get(url_en).text\r\n",
    "        parse_tree = bs4.BeautifulSoup(html_doc, 'html.parser')\r\n",
    "        entity_id = parse_tree.find(\"a\", {\"accesskey\": \"g\"})['href']\r\n",
    "        head_id, entity_id = os.path.split(entity_id)\r\n",
    "        return entity_id\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "        # print('Not found in: ', base_url)\r\n",
    "    entity_id = 'UNK'\r\n",
    "    return entity_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write a function to run the lookup and keep the resolved entities (only the resolved entities). You will call it `ne_ids_en`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "ne_ids_en = set()\r\n",
    "for entity in tqdm(ne_small_set):\r\n",
    "    entity_id = wikipedia_lookup(entity)\r\n",
    "    if entity_id != 'UNK':\r\n",
    "        ne_ids_en.add((entity, entity_id))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 76/76 [00:27<00:00,  2.72it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "ne_ids_en"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{(('KIM',), 'Q224736'),\n",
       " (('KLM', 'Royal', 'Dutch', 'Airlines'), 'Q181912'),\n",
       " (('KPMG', 'Peat', 'Marwick'), 'Q493751'),\n",
       " (('KTXL',), 'Q6339344'),\n",
       " (('Kabul',), 'Q5838'),\n",
       " (('Kaitaia',), 'Q257417'),\n",
       " (('Kajima',), 'Q1081154'),\n",
       " (('Kansas',), 'Q1558'),\n",
       " (('Kate', 'Michelman'), 'Q785671'),\n",
       " (('Katonah',), 'Q2888777'),\n",
       " (('Kawasaki', 'Steel'), 'Q6379829'),\n",
       " (('Kemper',), 'Q126993'),\n",
       " (('Kenneth', 'Abraham'), 'Q59268486'),\n",
       " (('Kenneth', 'H.', 'Olsen'), 'Q454315'),\n",
       " (('Kenton',), 'Q358393'),\n",
       " (('Kentucky',), 'Q1603'),\n",
       " (('Khost',), 'Q386682'),\n",
       " (('Kidder', 'Peabody'), 'Q3196386'),\n",
       " (('Kirin',), 'Q297659'),\n",
       " (('Kirin', 'Brewery'), 'Q13403399'),\n",
       " (('Kitchen',), 'Q43164'),\n",
       " (('Knoxville',), 'Q185582'),\n",
       " (('Kobe', 'Steel'), 'Q1730802'),\n",
       " (('Kodak',), 'Q486269'),\n",
       " (('Kraft', 'General', 'Foods'), 'Q327751'),\n",
       " (('Krenz',), 'Q21512656'),\n",
       " (('Kuala', 'Lumpur'), 'Q1865'),\n",
       " (('Kurds',), 'Q12223'),\n",
       " (('Kurt', 'Hager'), 'Q95367'),\n",
       " (('Kwek', 'Hong', 'Png'), 'Q10559663'),\n",
       " (('Ky',), 'Q225951'),\n",
       " (('Ky.',), 'Q225951'),\n",
       " (('Kyocera',), 'Q745099')}"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sometimes, entities need a confirmation. You will apply the same resolution with the Swedish wikipedia this time, `https://sv.wikipedia.org/wiki/`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "ne_ids_sv = set()\r\n",
    "for entity in tqdm(ne_small_set):\r\n",
    "    entity_id = wikipedia_lookup(entity, 'https://sv.wikipedia.org/wiki/')\r\n",
    "    if entity_id != 'UNK':\r\n",
    "        ne_ids_sv.add((entity, entity_id))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 76/76 [00:25<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "ne_ids_sv"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{(('KIM',), 'Q224736'),\n",
       " (('KLM', 'Royal', 'Dutch', 'Airlines'), 'Q181912'),\n",
       " (('Kabul',), 'Q5838'),\n",
       " (('Kansas',), 'Q1558'),\n",
       " (('Kenton',), 'Q358393'),\n",
       " (('Kentucky',), 'Q1603'),\n",
       " (('Khost',), 'Q386682'),\n",
       " (('Kirin',), 'Q297659'),\n",
       " (('Kitchen',), 'Q952431'),\n",
       " (('Knoxville',), 'Q232749'),\n",
       " (('Kodak',), 'Q486269'),\n",
       " (('Kraft', 'General', 'Foods'), 'Q12857502'),\n",
       " (('Kroger', 'Co'), 'Q153417'),\n",
       " (('Kuala', 'Lumpur'), 'Q1865')}"
      ]
     },
     "metadata": {},
     "execution_count": 136
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will compute the intersection of the two sets. You will assign it to a list that you will sort and that you will call: `confirmed_ne_en_sv`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "confirmed_ne_en_sv = list(ne_ids_en.intersection(ne_ids_sv))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "confirmed_ne_en_sv"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(('KLM', 'Royal', 'Dutch', 'Airlines'), 'Q181912'),\n",
       " (('Khost',), 'Q386682'),\n",
       " (('Kuala', 'Lumpur'), 'Q1865'),\n",
       " (('Kabul',), 'Q5838'),\n",
       " (('Kentucky',), 'Q1603'),\n",
       " (('KIM',), 'Q224736'),\n",
       " (('Kodak',), 'Q486269'),\n",
       " (('Kenton',), 'Q358393'),\n",
       " (('Kirin',), 'Q297659'),\n",
       " (('Kansas',), 'Q1558')]"
      ]
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first items in your list should look like:\n",
    "```\n",
    "[(('KIM',), 'Q224736'),\n",
    " (('KLM', 'Royal', 'Dutch', 'Airlines'), 'Q181912'),\n",
    " ...\n",
    "]\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submission"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When you have written all the code and run all the cells, fill in your ID and as well as the name of the notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "STIL_ID = [\"fi1231ka-s\", \"fr5536sj-s\"] # Write your stil ids as a list\r\n",
    "CURRENT_NOTEBOOK_PATH = os.path.join(os.getcwd(), \r\n",
    "                                     \"4-chunker.ipynb\") # Write the name of your notebook"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The submission code will send your answer. It consists of the baseline score, the improved machine-learning score, and the confirmed entities."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "source": [
    "import json\r\n",
    "ANSWER = json.dumps({'baseline_score': baseline_score,\r\n",
    "                    'improved_ml_score': improved_ml_score,\r\n",
    "                    'confirmed_ne_en_sv': confirmed_ne_en_sv})\r\n",
    "ANSWER"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'{\"baseline_score\": 0.770671072299583, \"improved_ml_score\": 0.9265266775640221, \"confirmed_ne_en_sv\": [[[\"KLM\", \"Royal\", \"Dutch\", \"Airlines\"], \"Q181912\"], [[\"Khost\"], \"Q386682\"], [[\"Kuala\", \"Lumpur\"], \"Q1865\"], [[\"Kabul\"], \"Q5838\"], [[\"Kentucky\"], \"Q1603\"], [[\"KIM\"], \"Q224736\"], [[\"Kodak\"], \"Q486269\"], [[\"Kenton\"], \"Q358393\"], [[\"Kirin\"], \"Q297659\"], [[\"Kansas\"], \"Q1558\"]]}'"
      ]
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the moment of truth:\n",
    "1. Save your notebook and\n",
    "2. Run the cells below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "SUBMISSION_NOTEBOOK_PATH = CURRENT_NOTEBOOK_PATH + \".submission.bz2\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "import bz2\r\n",
    "ASSIGNMENT = 4\r\n",
    "API_KEY = \"f581ba347babfea0b8f2c74a3a6776a7\"\r\n",
    "\r\n",
    "# Copy and compress current notebook\r\n",
    "with bz2.open(SUBMISSION_NOTEBOOK_PATH, mode=\"wb\") as fout:\r\n",
    "    with open(CURRENT_NOTEBOOK_PATH, \"rb\") as fin:\r\n",
    "        fout.write(fin.read())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "res = requests.post(\"https://vilde.cs.lth.se/edan20checker/submit\", \r\n",
    "                    files={\"notebook_file\": open(SUBMISSION_NOTEBOOK_PATH, \"rb\")}, \r\n",
    "                    data={\r\n",
    "                        \"stil_id\": STIL_ID,\r\n",
    "                        \"assignment\": ASSIGNMENT,\r\n",
    "                        \"answer\": ANSWER,\r\n",
    "                        \"api_key\": API_KEY,\r\n",
    "                    },\r\n",
    "               verify=True)\r\n",
    "\r\n",
    "# from IPython.display import display, JSON\r\n",
    "res.json()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'msg': None,\n",
       " 'status': 'correct',\n",
       " 'signature': '40aa81c7ca00273693fe1d08213202e9d3346f08825802f4fc9bc841964688d1806350ec03d55a5ae9b4674f02bcb8a13017a4231a26906337cc00967aed6bc3',\n",
       " 'submission_id': '3f0e4b26-482d-4d10-90ce-34616a2d5029'}"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Turning in your assignment"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now your are done with the program. To complete this assignment, you will:\n",
    "1. Write a short individual report on your program. Do not forget to answer all the question in the notebook.\n",
    "2. Read the article, <a href=\"https://www.aclweb.org/anthology/C18-1139\"><i>Contextual String Embeddings for Sequence Labeling</i></a> by Akbik et al. (2018) and outline the main differences between their system and yours. A LSTM is a type of recurrent neural network, while CRF is a sort of beam search. You will tell the performance they reach on the corpus you used in this laboratory.\n",
    "\n",
    "Submit your report as well as your notebook (for archiving purposes) to Canvas: https://canvas.education.lu.se/. To write your report, you can either\n",
    "1. Write directly your text in Canvas, or\n",
    "2. Use Latex and Overleaf (www.overleaf.com). This will probably help you structure your text. You will then upload a PDF file in Canvas.\n",
    "\n",
    "The submission deadline is October 8, 2021."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "1b01d829dac8ca1a8f35a8697f342666f6cab48c37e2c1e337b0d17d76583aaf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}